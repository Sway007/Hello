{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give a brief description of neural network including basic mathematic knowledge, important branches, algorithms and models.\n",
    "\n",
    ">_\"a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.\"_\n",
    " \n",
    "    In \"Neural Network Primer: Part I\" by Maureen Caudill, AI Expert, Feb. 1989\n",
    "    \n",
    "Neural neworks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output as shown in the graphic below.\n",
    "\n",
    "<img src=\"./imgs/nn_schematic.gif\" />\n",
    "\n",
    "Usaully, the node is called \"neuron\", and the activation function is represented as<br>\n",
    "$$S(Wx + b) = \\frac{1}{1+e^{-(Wx+b)}} = \\frac{e^{Wx+b}}{e^{Wx+b} + 1}$$\n",
    "\n",
    "each \"neuron\" forward propagation its output to next layer, eventually, we get outcome from the ouput layer.\n",
    "\n",
    "### Try to explain clearly what BP (back propagation) is and give a conclusion of the derivation process.\n",
    "\n",
    "BP algorithm is somewhat optimize the partial derivatitve process. Consider the below situation:\n",
    "<img src=\"./imgs/bp1.jpg\"/>\n",
    "according to the chain rule we get:\n",
    "$$\\frac {\\partial e}{\\partial a} = \\frac {\\partial e}{\\partial c}*\\frac {\\partial c}{\\partial a}\\quad and\\quad  \n",
    "\\frac{\\partial e}{\\partial b}=\\frac {\\partial e}{\\partial c}*\\frac{\\partial c}{\\partial b}+\n",
    "\\frac{\\partial e}{\\partial d}*\\frac{\\partial d}{\\partial b}$$\n",
    "\n",
    "Start from the top \"node\" e, process derivative by unit layer. we diffirentiate with repect to each child node(in the second layer), and store the derivative in the node respectively. in the second layer, we sum all the derivatives stored in each node, then the result is the partial derivative respect to the current node. repeat the procejures, we can get all partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
